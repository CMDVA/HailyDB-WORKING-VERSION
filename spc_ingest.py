"""
Storm Prediction Center (SPC) Report Ingestion Service
Handles parsing of multi-section CSV files and cross-referencing with NWS alerts
"""

import csv
import logging
import requests
import hashlib
import os
from datetime import datetime, date, timedelta
from typing import Dict, List, Optional, Tuple, Iterator
from io import StringIO
import re

from config import Config
from models import SPCReport, SPCIngestionLog, Alert, db
from sqlalchemy import and_, or_, func
from sqlalchemy.exc import IntegrityError

logger = logging.getLogger(__name__)

def chunks(data: List, chunk_size: int) -> Iterator[List]:
    """Utility function to chunk data into batches"""
    for i in range(0, len(data), chunk_size):
        yield data[i:i + chunk_size]

class SPCIngestService:
    """
    SPC Report Ingestion Service
    Handles variable polling schedules and CSV parsing with header detection
    """
    
    def __init__(self, db_session):
        self.db = db_session
        self.base_url = "https://www.spc.noaa.gov/climo/reports/"
        self.db_write_batch_size = int(os.getenv("DB_WRITE_BATCH_SIZE", "500"))
        
    def get_polling_schedule(self, report_date: date) -> int:
        """
        Systematic polling schedule with data protection
        Returns interval in minutes
        """
        today = date.today()
        days_ago = (today - report_date).days
        
        if days_ago == 0:
            return 5  # Every 5 minutes for today (T-0)
        elif 1 <= days_ago <= 4:
            return 30  # Every 30 minutes for T-1 through T-4
        elif 5 <= days_ago <= 7:
            return 60  # Hourly updates for T-5 through T-7
        elif 8 <= days_ago <= 15:
            return 1440  # Daily updates for T-8 through T-15
        else:
            return None  # Data protected at T-16+, no automatic polling
    
    def should_poll_now(self, report_date: date) -> bool:
        """
        Check if we should poll for this date based on systematic schedule
        Uses SPC Day logic for T-0 determination and includes data protection for T-16+ dates
        """
        # Use SPC Day logic to determine current day and days_ago calculation
        now_utc = datetime.utcnow()
        
        if now_utc.hour >= 12:
            # Current time is >= 12:00Z, so SPC day is today
            current_spc_day = now_utc.date()
        else:
            # Current time is < 12:00Z, so SPC day is yesterday
            current_spc_day = (now_utc - timedelta(days=1)).date()
        
        days_ago = (current_spc_day - report_date).days
        
        if days_ago >= 16:
            return False  # Data protected, no automatic polling
        
        # Check if we already have substantial data for this date
        existing_count = SPCReport.query.filter(
            SPCReport.report_date == report_date
        ).count()
        
        # For T-0, T-1, T-2 (most current SPC Days), use 5-minute polling for real-time updates
        if days_ago <= 2:
            last_log = SPCIngestionLog.query.filter(
                SPCIngestionLog.report_date == report_date,
                SPCIngestionLog.success == True
            ).order_by(SPCIngestionLog.completed_at.desc()).first()
            
            if not last_log:
                return True  # Never polled before
                
            # For T-0 through T-2, poll every 5 minutes for real-time updates regardless of existing count
            time_since_last = datetime.utcnow() - last_log.completed_at
            return time_since_last.total_seconds() >= (5 * 60)  # 5 minutes for T-0, T-1, T-2
        
        # For dates older than 7 days, skip if we have any data (likely complete)
        if days_ago >= 7 and existing_count > 0:
            return False
            
        # For recent dates (T-1 to T-6), check polling interval
        last_log = SPCIngestionLog.query.filter(
            SPCIngestionLog.report_date == report_date,
            SPCIngestionLog.success == True
        ).order_by(SPCIngestionLog.completed_at.desc()).first()
        
        if not last_log:
            return True  # Never polled before
            
        interval_minutes = self.get_polling_schedule(report_date)
        if interval_minutes is None:
            return False  # No polling schedule defined
            
        time_since_last = datetime.utcnow() - last_log.completed_at
        
        # Only re-poll if interval has passed - allow updates to existing data
        interval_passed = time_since_last.total_seconds() >= (interval_minutes * 60)
        
        return interval_passed  # Allow re-polling to capture additional reports
    
    def is_backfill_candidate(self, report_date: date) -> bool:
        """
        Check if date needs backfill processing (overrides T-16 protection)
        Returns True for dates with no data or failed ingestions
        """
        # Check if we have any successful ingestion for this date
        successful_log = SPCIngestionLog.query.filter(
            SPCIngestionLog.report_date == report_date,
            SPCIngestionLog.success == True
        ).first()
        
        if not successful_log:
            return True  # No successful ingestion ever
        
        # Check if we have actual SPC reports for this date
        report_count = SPCReport.query.filter(
            SPCReport.report_date == report_date
        ).count()
        
        if report_count == 0:
            return True  # No reports despite successful log (empty day)
        
        return False  # Has data, not a backfill candidate
    
    def force_poll_for_backfill(self, report_date: date, reason: str = "manual_backfill") -> Dict:
        """
        Force polling for backfill processing (bypasses all scheduling rules)
        Used for missing data recovery and manual corrections
        """
        logger.info(f"Force polling {report_date} for backfill: {reason}")
        return self.poll_spc_reports(report_date)
    
    def format_date_for_url(self, report_date: date) -> str:
        """Convert date to YYMMDD format for SPC URL"""
        return report_date.strftime("%y%m%d")
    
    def poll_spc_reports(self, report_date: date = None) -> Dict:
        """
        Main method to poll SPC reports for a given date
        """
        if not report_date:
            report_date = date.today()
            
        # Check if we should poll based on schedule
        if not self.should_poll_now(report_date):
            return {
                'status': 'skipped',
                'message': f'Not time to poll {report_date} yet'
            }
        
        # Create ingestion log
        log = SPCIngestionLog()
        log.report_date = report_date
        log.started_at = datetime.utcnow()
        self.db.add(log)
        self.db.flush()  # Get the ID
        
        try:
            url = f"{self.base_url}{self.format_date_for_url(report_date)}_rpts_filtered.csv"
            log.url_attempted = url
            
            logger.info(f"Polling SPC reports from {url}")
            
            # Download CSV with proper headers to get complete data
            headers = {
                'User-Agent': 'HailyDB-SPC-Ingestion/2.0 (contact@hailydb.com)',
                'Accept': 'text/csv,text/plain,*/*',
                'Accept-Encoding': 'identity',  # Disable compression to avoid truncation
                'Connection': 'keep-alive'
            }
            response = requests.get(url, headers=headers, timeout=Config.REQUEST_TIMEOUT)
            logger.info(f"Response status: {response.status_code}")
            logger.info(f"Response headers: {dict(response.headers)}")
            logger.info(f"Response encoding: {response.encoding}")
            response.raise_for_status()
            
            # Check response content
            logger.info(f"Raw response length: {len(response.content)} bytes")
            logger.info(f"Text response length: {len(response.text)} characters")
            
            if not response.content:
                raise Exception("Empty response content received from SPC")
            
            if not response.text.strip():
                raise Exception("Empty text content after decoding")
            
            logger.info(f"First 500 chars: {repr(response.text[:500])}")
            
            # Sanitize CSV content - remove null characters that cause PostgreSQL errors
            clean_content = response.text.replace('\x00', '')
            if len(clean_content) != len(response.text):
                logger.warning(f"Removed {len(response.text) - len(clean_content)} null characters from CSV")
            
            # Parse CSV content
            result = self._parse_spc_csv(clean_content, report_date)
            
            # Check if we have more reports than before
            existing_count = SPCReport.query.filter(
                SPCReport.report_date == report_date
            ).count()
            
            logger.info(f"Found {result['total_reports']} reports in CSV, database has {existing_count}")
            
            if result['total_reports'] == 0:
                logger.warning(f"No reports parsed from CSV for {report_date}")
                log.success = True
                log.completed_at = datetime.utcnow()
                log.total_reports = 0
                self.db.commit()
                return {
                    'status': 'no_data_in_csv',
                    'existing_count': existing_count,
                    'message': f'No reports found in CSV for {report_date}'
                }
            
            # Store new reports
            stored_counts = self._store_reports(result['reports'], report_date)
            
            # Update log
            log.success = True
            log.completed_at = datetime.utcnow()
            log.tornado_reports = stored_counts['tornado']
            log.wind_reports = stored_counts['wind'] 
            log.hail_reports = stored_counts['hail']
            log.total_reports = sum(stored_counts.values())
            
            self.db.commit()
            
            logger.info(f"Successfully ingested {log.total_reports} SPC reports for {report_date}")
            
            return {
                'status': 'success',
                'date': report_date.isoformat(),
                'tornado_reports': stored_counts['tornado'],
                'wind_reports': stored_counts['wind'],
                'hail_reports': stored_counts['hail'],
                'total_reports': log.total_reports
            }
            
        except Exception as e:
            logger.error(f"Error ingesting SPC reports for {report_date}: {e}")
            log.error_message = str(e)
            log.completed_at = datetime.utcnow()
            self.db.commit()
            raise
    
    def _parse_spc_csv(self, csv_content: str, report_date: date) -> Dict:
        """
        Parse multi-section SPC CSV with comprehensive malformation handling
        Returns dict with parsed reports by type
        """
        # Pre-process CSV to handle multi-line records and truncated lines
        processed_content = self._preprocess_csv_content(csv_content)
        lines = processed_content.strip().split('\n')
        
        reports = []
        current_section = None
        current_headers = None
        
        tornado_count = wind_count = hail_count = 0
        failed_lines = []
        
        for line_num, line in enumerate(lines):
            if not line.strip():
                continue
                
            # Detect section headers
            if self._is_header_line(line):
                current_section, current_headers = self._parse_header_line(line)
                logger.debug(f"Found {current_section} section at line {line_num + 1}")
                continue
            
            # Parse data lines - ensure every valid data line is processed
            if current_section and current_headers and len(line) >= 4 and line[:4].isdigit():
                report = None
                
                # Primary parser with forced success for valid CSV structure
                try:
                    report = self._parse_report_line(
                        line, current_section, current_headers, report_date, line_num + 1
                    )
                except Exception as e:
                    logger.warning(f"Primary parser failed line {line_num + 1}: {e}")
                
                # Aggressive fallback for any line that starts with time
                if not report:
                    try:
                        report = self._force_parse_valid_line(
                            line, current_section, report_date, line_num + 1
                        )
                        if report:
                            logger.info(f"Force parser recovered line {line_num + 1}")
                    except Exception as e:
                        logger.error(f"Force parser failed line {line_num + 1}: {e}")
                
                # Store results - every time-starting line must be captured
                if report:
                    reports.append(report)
                    if current_section == 'tornado':
                        tornado_count += 1
                    elif current_section == 'wind':
                        wind_count += 1
                    elif current_section == 'hail':
                        hail_count += 1
                else:
                    failed_lines.append((line_num + 1, line[:100]))
                    logger.error(f"CRITICAL: Lost data record at line {line_num + 1}: {line[:100]}")
        
        logger.info(f"CSV parsing complete: {len(lines)} total lines processed")
        logger.info(f"Sections detected: tornado={tornado_count}, wind={wind_count}, hail={hail_count}")
        logger.info(f"Total reports parsed: {len(reports)}")
        logger.info(f"Failed to parse {len(failed_lines)} lines")
        
        return {
            'reports': reports,
            'total_reports': len(reports),
            'tornado_count': tornado_count,
            'wind_count': wind_count,
            'hail_count': hail_count,
            'failed_lines': failed_lines
        }
    
    def _preprocess_csv_content(self, csv_content: str) -> str:
        """Pre-process CSV to handle multi-line records and section transitions"""
        lines = csv_content.split('\n')
        processed_lines = []
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            # Skip empty lines
            if not line:
                i += 1
                continue
            
            # Keep header lines as-is
            if self._is_header_line(line):
                processed_lines.append(line)
                i += 1
                continue
            
            # Check if line looks like a valid data record (starts with 4-digit time)
            if len(line) >= 4 and line[:4].isdigit():
                # Valid data line - check if it has enough commas
                if line.count(',') >= 6:
                    processed_lines.append(line)
                    i += 1
                else:
                    # Try to merge with continuation lines
                    merged_line = line
                    j = i + 1
                    while j < len(lines) and merged_line.count(',') < 6:
                        next_line = lines[j].strip()
                        if (next_line and not self._is_header_line(next_line) and 
                            not (len(next_line) >= 4 and next_line[:4].isdigit())):
                            merged_line += " " + next_line
                            j += 1
                        else:
                            break
                    
                    processed_lines.append(merged_line)
                    i = j
            else:
                # Skip malformed lines that don't start with time
                logger.debug(f"Skipping malformed line {i+1}: {line[:50]}")
                i += 1
        
        return '\n'.join(processed_lines)
    
    def _aggressive_recovery_parse(self, line: str, section_type: str, report_date: date, line_num: int) -> Optional[Dict]:
        """Final attempt parser using minimal field extraction"""
        try:
            line = line.strip()
            if not line or len(line) < 10:
                return None
            
            # Extract any recognizable patterns with regex
            import re
            
            # Pattern to match time (4 digits), magnitude, and basic location info
            basic_pattern = r'^(\d{4}),([^,]*),([^,]+)'
            match = re.match(basic_pattern, line)
            
            if not match:
                return None
            
            time_val, mag_val, location_val = match.groups()
            
            # Extract additional fields using simple parsing
            remaining = line[match.end():]
            parts = remaining.split(',')
            
            # Build minimal viable record
            report = {
                'report_date': report_date,
                'report_type': section_type,
                'time_utc': time_val.strip(),
                'location': location_val.strip(),
                'county': parts[0].strip() if len(parts) > 0 else 'Unknown',
                'state': parts[1].strip() if len(parts) > 1 else 'UNK',
                'latitude': None,
                'longitude': None,
                'comments': ','.join(parts[4:]).strip() if len(parts) > 4 else '',
                'magnitude': self._parse_magnitude(mag_val.strip(), section_type),
                'raw_csv_line': line
            }
            
            # Try to extract coordinates
            for part in parts:
                try:
                    val = float(part.strip())
                    if 20 <= abs(val) <= 90 and report['latitude'] is None:
                        report['latitude'] = val
                    elif 60 <= abs(val) <= 180 and report['longitude'] is None:
                        report['longitude'] = val
                except ValueError:
                    continue
            
            return report
            
        except Exception as e:
            logger.debug(f"Aggressive recovery failed line {line_num}: {e}")
            return None
    
    def _force_parse_valid_line(self, line: str, section_type: str, report_date: date, line_num: int) -> Optional[Dict]:
        """Force parse any line that starts with 4-digit time - guaranteed success for valid CSV"""
        try:
            line = line.strip()
            if not line or len(line) < 4 or not line[:4].isdigit():
                return None
            
            # Split on commas - SPC CSV has exactly 8 fields: Time,Mag,Location,County,State,Lat,Lon,Comments
            parts = line.split(',')
            if len(parts) < 7:
                return None
            
            # Force extract the 8 standard fields
            time_field = parts[0].strip()
            magnitude_field = parts[1].strip()
            location_field = parts[2].strip()
            county_field = parts[3].strip()
            state_field = parts[4].strip()
            
            # Extract coordinates with error handling
            latitude = None
            longitude = None
            try:
                latitude = float(parts[5].strip()) if parts[5].strip() else None
            except ValueError:
                pass
            try:
                longitude = float(parts[6].strip()) if parts[6].strip() else None
            except ValueError:
                pass
            
            # Comments field - everything after the 7th comma
            comments_field = ','.join(parts[7:]).strip() if len(parts) > 7 else ''
            
            # Build complete report structure
            report = {
                'report_date': report_date,
                'report_type': section_type,
                'time_utc': time_field,
                'location': location_field,
                'county': county_field,
                'state': state_field,
                'latitude': latitude,
                'longitude': longitude,
                'comments': comments_field,
                'magnitude': self._parse_magnitude(magnitude_field, section_type),
                'raw_csv_line': line
            }
            
            return report
            
        except Exception as e:
            logger.error(f"Force parser critical failure line {line_num}: {e}")
            return None
    
    def _parse_magnitude(self, mag_str: str, section_type: str) -> dict:
        """Parse magnitude field based on section type"""
        try:
            if section_type == 'tornado':
                return {'f_scale': mag_str} if mag_str != 'UNK' else {}
            elif section_type == 'wind':
                if mag_str == 'UNK':
                    return {'speed_text': 'UNK', 'speed': None}
                try:
                    speed = int(mag_str)
                    return {'speed': speed}
                except ValueError:
                    return {'speed_text': mag_str, 'speed': None}
            elif section_type == 'hail':
                try:
                    size = int(mag_str)
                    return {'size_hundredths': size, 'size_inches': size / 100.0}
                except ValueError:
                    return {}
            return {}
        except Exception:
            return {}
    
    def _is_header_line(self, line: str) -> bool:
        """Check if line is a section header"""
        # Look for Time header with section-specific columns
        if not line.startswith('Time,'):
            return False
        return any(indicator in line for indicator in ['F_Scale', 'Speed', 'Size'])
    
    def _parse_header_line(self, line: str) -> Tuple[str, List[str]]:
        """Parse header line to determine section type and column names"""
        headers = [h.strip() for h in line.split(',')]
        
        if 'F_Scale' in line:
            return 'tornado', headers
        elif 'Speed' in line:
            return 'wind', headers
        elif 'Size' in line:
            return 'hail', headers
        else:
            return 'unknown', headers
    
    def _emergency_parse_line(self, line: str, section_type: str, report_date: date, line_num: int) -> Optional[Dict]:
        """
        Emergency fallback parser for critically malformed SPC CSV lines
        Uses aggressive pattern matching to extract essential data
        """
        try:
            line = line.strip()
            if not line:
                return None
            
            # Split by comma and extract what we can
            parts = [p.strip() for p in line.split(',')]
            
            if len(parts) < 4:  # Absolute minimum: Time, Mag, Location, County
                return None
            
            # Emergency field extraction with defaults
            time_field = parts[0] if parts[0] else "0000"
            magnitude_field = parts[1] if len(parts) > 1 else "UNK"
            location_field = parts[2] if len(parts) > 2 else "Unknown"
            
            # Extract county, state, coordinates with fallbacks
            county_field = "Unknown"
            state_field = "UNK"
            latitude = None
            longitude = None
            comments = ""
            
            # Scan for recognizable patterns
            for i, part in enumerate(parts):
                # Look for state codes (2-letter uppercase)
                if len(part) == 2 and part.isalpha() and part.isupper():
                    state_field = part
                    if i > 0:
                        county_field = parts[i-1]
                
                # Look for coordinates (numeric with decimal)
                try:
                    float_val = float(part)
                    if -180 <= float_val <= 180:
                        if latitude is None and 20 <= abs(float_val) <= 90:
                            latitude = float_val
                        elif longitude is None and 60 <= abs(float_val) <= 180:
                            longitude = float_val
                except ValueError:
                    continue
            
            # Join remaining parts as comments
            if len(parts) > 7:
                comments = ','.join(parts[7:])
            
            # Create magnitude structure based on section type
            if section_type == 'tornado':
                magnitude = {'f_scale': magnitude_field} if magnitude_field != 'UNK' else {}
            elif section_type == 'wind':
                if magnitude_field == 'UNK':
                    magnitude = {'speed_text': 'UNK', 'speed': None}
                else:
                    try:
                        speed = int(magnitude_field)
                        magnitude = {'speed': speed}
                    except ValueError:
                        magnitude = {'speed_text': magnitude_field, 'speed': None}
            elif section_type == 'hail':
                try:
                    size = int(magnitude_field)
                    magnitude = {
                        'size_hundredths': size,
                        'size_inches': size / 100.0
                    }
                except ValueError:
                    magnitude = {}
            else:
                magnitude = {}
            
            # Build report structure
            report = {
                'report_date': report_date,
                'report_type': section_type,
                'time_utc': time_field,
                'location': location_field,
                'county': county_field,
                'state': state_field,
                'latitude': latitude,
                'longitude': longitude,
                'comments': comments,
                'magnitude': magnitude,
                'raw_csv_line': line
            }
            
            return report
            
        except Exception as e:
            logger.error(f"Emergency parser failed on line {line_num}: {e}")
            return None
    
    def _parse_report_line(self, line: str, section_type: str, headers: List[str], 
                          report_date: date, line_num: int) -> Optional[Dict]:
        """Bulletproof SPC CSV parser with comprehensive error recovery"""
        try:
            line = line.strip()
            if not line:
                return None
            
            # COMPREHENSIVE SPC CSV PARSING STRATEGY
            # SPC CSV has multiple malformation patterns that must be handled systematically
            
            # Step 1: Handle the most common pattern - unquoted commas in comments field
            # Strategy: Find first 7 comma positions, everything after is comments
            comma_positions = []
            for i, char in enumerate(line):
                if char == ',':
                    comma_positions.append(i)
                if len(comma_positions) >= 7:
                    break
            
            # Extract fields based on comma positions
            if len(comma_positions) >= 6:  # Need at least 6 commas for 7 fields + comments
                values = []
                start = 0
                
                # Extract first 7 fields
                for pos in comma_positions[:6]:
                    values.append(line[start:pos].strip())
                    start = pos + 1
                
                # Extract 7th field (before 7th comma if it exists)
                if len(comma_positions) >= 7:
                    values.append(line[start:comma_positions[6]].strip())
                    # Everything after 7th comma is comments
                    values.append(line[comma_positions[6]+1:].strip())
                else:
                    # No 7th comma - rest is the 7th field, empty comments
                    values.append(line[start:].strip())
                    values.append('')
            else:
                # Fallback for lines with insufficient commas
                parts = line.split(',')
                if len(parts) < 7:
                    logger.warning(f"Insufficient fields at line {line_num}: {len(parts)} < 7")
                    return None
                
                values = parts[:7]
                if len(parts) > 7:
                    values.append(','.join(parts[7:]))
                else:
                    values.append('')
            
            # Step 2: Handle extra state field malformation
            # Pattern: Time,Mag,Location,ExtraState,County,State,Lat,Lon,Comments
            # Detect by checking if field 3 is a 2-letter state code
            if (len(values) >= 8 and len(line.split(',')) >= 9 and 
                len(values[3]) == 2 and values[3].isalpha() and values[3].isupper()):
                
                # Merge location with extra state
                parts = line.split(',')
                values = [
                    parts[0].strip(),  # Time
                    parts[1].strip(),  # Magnitude
                    f"{parts[2]} {parts[3]}".strip(),  # Location + ExtraState
                    parts[4].strip(),  # County
                    parts[5].strip(),  # State
                    parts[6].strip(),  # Lat
                    parts[7].strip(),  # Lon
                    ','.join(parts[8:]).strip() if len(parts) > 8 else ''  # Comments
                ]
                logger.debug(f"Merged extra state field at line {line_num}")
            
            # Step 3: Validate and normalize
            values = [str(v).strip() for v in values]
            
            # Ensure exactly the right number of fields
            while len(values) < len(headers):
                values.append('')
            values = values[:len(headers)]
            
            if len(values) != len(headers):
                logger.error(f"Field count mismatch at line {line_num}: {len(values)} vs {len(headers)}")
                return None
            
            # Create column mapping
            data = dict(zip(headers, values))
            
            # Extract common fields
            report = {
                'report_date': report_date,
                'report_type': section_type,
                'time_utc': data.get('Time', '').strip(),
                'location': data.get('Location', '').strip(),
                'county': data.get('County', '').strip(),
                'state': data.get('State', '').strip(),
                'comments': data.get('Comments', '').strip(),
                'raw_csv_line': line.strip()
            }
            
            # Parse coordinates
            try:
                report['latitude'] = float(data.get('Lat', 0)) if data.get('Lat') else None
                report['longitude'] = float(data.get('Lon', 0)) if data.get('Lon') else None
            except (ValueError, TypeError):
                report['latitude'] = None
                report['longitude'] = None
            
            # Parse magnitude based on section type
            if section_type == 'tornado':
                magnitude = data.get('F_Scale', '').strip()
                report['magnitude'] = {'f_scale': magnitude} if magnitude else {}
            elif section_type == 'wind':
                speed_raw = data.get('Speed', '').strip()
                if speed_raw == 'UNK':
                    # Store UNK as string field to avoid PostgreSQL JSONB validation errors
                    report['magnitude'] = {'speed_text': 'UNK', 'speed': None}
                elif speed_raw:
                    try:
                        speed = int(speed_raw)
                        report['magnitude'] = {'speed': speed}
                    except (ValueError, TypeError):
                        report['magnitude'] = {}
                else:
                    report['magnitude'] = {}
            elif section_type == 'hail':
                size_raw = data.get('Size', '').strip()
                if size_raw == 'UNK':
                    # Store UNK as string field to avoid PostgreSQL JSONB validation errors
                    report['magnitude'] = {'size_text': 'UNK', 'size': None}
                elif size_raw:
                    try:
                        size_hundredths = int(size_raw)
                        size_inches = size_hundredths / 100.0
                        report['magnitude'] = {'size_hundredths': size_hundredths, 'size_inches': size_inches}
                    except (ValueError, TypeError):
                        report['magnitude'] = {}
                else:
                    report['magnitude'] = {}
            
            # Generate hash for duplicate detection including report_type
            # This ensures tornado and wind reports at same location/time are treated as separate
            # Normalize empty values to ensure consistent hashing
            lat_str = str(report.get('latitude', '')) if report.get('latitude') is not None else ''
            lon_str = str(report.get('longitude', '')) if report.get('longitude') is not None else ''
            mag_str = str(report['magnitude']) if report['magnitude'] else '{}'
            
            hash_data = f"{report['report_date']}|{report['report_type']}|{report['time_utc']}|{report['location']}|{report['county']}|{report['state']}|{lat_str}|{lon_str}|{mag_str}"
            
            # Clean the hash data to remove any problematic characters
            clean_hash_data = hash_data.replace('\x00', '').replace('\r', '').replace('\n', ' ')
            report['row_hash'] = hashlib.sha256(clean_hash_data.encode('utf-8')).hexdigest()
            
            # Debug: log hash data for troubleshooting
            if report['report_type'] == 'tornado' and len(report.get('location', '')) > 0:
                logger.debug(f"Hash data for tornado: {clean_hash_data[:100]}... -> {report['row_hash'][:16]}...")
            
            return report
            
        except Exception as e:
            logger.error(f"Error parsing report line {line_num}: {e}")
            return None
    
    def _store_reports(self, reports: List[Dict], report_date: date, is_reimport: bool = False) -> Dict[str, int]:
        """Store parsed reports with individual record insertion to prevent batch failures"""
        counts = {'tornado': 0, 'wind': 0, 'hail': 0}
        duplicates_skipped = 0
        errors_count = 0
        batch_size = self.db_write_batch_size
        
        # Process reports in batches for performance
        for i in range(0, len(reports), batch_size):
            batch = reports[i:i + batch_size]
            successful_in_batch = 0
            
            # Individual record processing - prevents entire batch rollback
            for report_data in batch:
                try:
                    # Prepare magnitude data
                    magnitude_data = report_data['magnitude']
                    if isinstance(magnitude_data, str):
                        import json
                        try:
                            magnitude_data = json.loads(magnitude_data)
                        except (json.JSONDecodeError, TypeError):
                            magnitude_data = {}
                    
                    # Check for location-based duplicates first
                    existing_location_duplicate = None
                    if report_data.get('latitude') and report_data.get('longitude'):
                        existing_location_duplicate = SPCReport.query.filter(
                            SPCReport.report_date == report_data['report_date'],
                            SPCReport.report_type == report_data['report_type'],
                            SPCReport.latitude == report_data['latitude'],
                            SPCReport.longitude == report_data['longitude'],
                            SPCReport.location == report_data['location']
                        ).first()
                    
                    if existing_location_duplicate:
                        # True location-based duplicate - skip
                        duplicates_skipped += 1
                        logger.debug(f"Location duplicate skipped: {report_data['location']} at {report_data['latitude']}, {report_data['longitude']}")
                        continue
                    
                    # Create SPCReport object
                    report = SPCReport()
                    report.report_date = report_data['report_date']
                    report.report_type = report_data['report_type']
                    report.time_utc = report_data['time_utc']
                    report.location = report_data['location']
                    report.county = report_data['county']
                    report.state = report_data['state']
                    report.latitude = report_data['latitude']
                    report.longitude = report_data['longitude']
                    report.comments = report_data['comments']
                    report.magnitude = magnitude_data
                    report.raw_csv_line = report_data['raw_csv_line']
                    report.row_hash = report_data['row_hash']
                    
                    # Add and flush individual record
                    self.db.add(report)
                    self.db.flush()
                    
                    # Auto-enrichment disabled during batch processing to prevent database connection crashes
                    # Enrichment can be triggered separately via dashboard after successful ingestion
                    logger.debug(f"Stored SPC report {report.id} without auto-enrichment (prevents connection crashes)")
                    
                    # Success - increment counters
                    successful_in_batch += 1
                    counts[report_data['report_type']] += 1
                    
                except IntegrityError as ie:
                    # Check if this is a hash constraint violation for the same date
                    self.db.rollback()
                    if 'uq_spc_report_hash' in str(ie) or 'duplicate key value violates unique constraint' in str(ie):
                        # Check if duplicate exists for the same date
                        existing_same_date = SPCReport.query.filter_by(
                            row_hash=report_data['row_hash'],
                            report_date=report_data['report_date']
                        ).first()
                        
                        if existing_same_date:
                            # True duplicate for same date - skip
                            duplicates_skipped += 1
                            logger.debug(f"Duplicate skipped (same date): {report_data['row_hash'][:16]}...")
                        else:
                            # Hash collision with different date - allow insertion by updating hash
                            import time
                            report_data['row_hash'] = report_data['row_hash'] + f"_{int(time.time())}"
                            
                            # Retry with modified hash
                            report = SPCReport()
                            report.report_date = report_data['report_date']
                            report.report_type = report_data['report_type']
                            report.time_utc = report_data['time_utc']
                            report.location = report_data['location']
                            report.county = report_data['county']
                            report.state = report_data['state']
                            report.latitude = report_data['latitude']
                            report.longitude = report_data['longitude']
                            report.comments = report_data['comments']
                            report.magnitude = magnitude_data
                            report.raw_csv_line = report_data['raw_csv_line']
                            report.row_hash = report_data['row_hash']
                            
                            try:
                                self.db.add(report)
                                self.db.flush()
                                
                                # Auto-enrichment disabled during batch processing to prevent database connection crashes  
                                # Enrichment can be triggered separately via dashboard after successful ingestion
                                logger.debug(f"Stored SPC report {report.id} without auto-enrichment (retry - prevents crashes)")
                                
                                successful_in_batch += 1
                                counts[report_data['report_type']] += 1
                                logger.debug(f"Inserted with modified hash: {report_data['row_hash'][:16]}...")
                            except Exception as retry_error:
                                self.db.rollback()
                                errors_count += 1
                                logger.error(f"Failed retry with modified hash: {retry_error}")
                    else:
                        errors_count += 1
                        logger.warning(f"Non-hash constraint violation: {ie}")
                        
                except Exception as e:
                    # Individual rollback preserves other successful records
                    self.db.rollback()
                    errors_count += 1
                    logger.error(f"Failed to insert record: {e}")
                    logger.error(f"Failed record data: {report_data['raw_csv_line']}")
                    logger.error(f"Report data: {report_data}")
                    continue
            
            # Commit successful records in batch
            try:
                if successful_in_batch > 0:
                    self.db.commit()
                    logger.info(f"Batch {i//batch_size + 1}: stored {successful_in_batch}/{len(batch)} reports")
            except Exception as e:
                self.db.rollback()
                logger.error(f"Error committing batch {i//batch_size + 1}: {e}")
                # Reset counts for failed commit
                for _ in range(successful_in_batch):
                    for report_type in counts:
                        counts[report_type] = max(0, counts[report_type] - 1)
        
        total_stored = sum(counts.values())
        logger.info(f"Successfully stored {total_stored} total reports for {report_date}")
        if duplicates_skipped > 0:
            logger.info(f"Skipped {duplicates_skipped} duplicate records")
        if errors_count > 0:
            logger.warning(f"Failed to insert {errors_count} records due to errors")
        
        return {
            'tornado': counts['tornado'],
            'wind': counts['wind'], 
            'hail': counts['hail'],
            'duplicates_skipped': duplicates_skipped,
            'errors_count': errors_count,
            'total_stored': total_stored
        }
    
    def get_ingestion_stats(self) -> Dict:
        """Get SPC ingestion statistics"""
        total_reports = SPCReport.query.count()
        
        # Reports by type
        type_counts = self.db.query(
            SPCReport.report_type,
            func.count(SPCReport.id).label('count')
        ).group_by(SPCReport.report_type).all()
        
        # Recent ingestion logs
        recent_logs = SPCIngestionLog.query.order_by(
            SPCIngestionLog.started_at.desc()
        ).limit(10).all()
        
        return {
            'total_reports': total_reports,
            'reports_by_type': {row.report_type: row.count for row in type_counts},
            'recent_ingestions': [
                {
                    'date': log.report_date.isoformat(),
                    'success': log.success,
                    'total_reports': log.total_reports,
                    'started_at': log.started_at.isoformat() if log.started_at else None,
                    'error': log.error_message
                }
                for log in recent_logs
            ]
        }

    def reimport_spc_reports(self, report_date: date) -> Dict:
        """
        Reimport SPC reports for a specific date (bypass duplicate detection)
        Used by the reimport endpoint to ensure complete data replacement
        """
        # Create ingestion log
        log = SPCIngestionLog()
        log.report_date = report_date
        log.started_at = datetime.utcnow()
        self.db.add(log)
        self.db.flush()
        
        try:
            url = f"{self.base_url}{self.format_date_for_url(report_date)}_rpts_filtered.csv"
            log.url_attempted = url
            
            logger.info(f"Reimporting SPC reports from {url}")
            
            # Download CSV with proper headers
            headers = {
                'User-Agent': 'HailyDB-SPC-Ingestion/2.0 (contact@hailydb.com)',
                'Accept': 'text/csv,text/plain,*/*',
                'Accept-Encoding': 'identity',
                'Connection': 'keep-alive'
            }
            response = requests.get(url, headers=headers, timeout=Config.REQUEST_TIMEOUT)
            response.raise_for_status()
            
            # Sanitize CSV content
            clean_content = response.text.replace('\x00', '')
            
            # Parse CSV content
            result = self._parse_spc_csv(clean_content, report_date)
            
            if result['total_reports'] == 0:
                log.success = True
                log.completed_at = datetime.utcnow()
                log.total_reports = 0
                self.db.commit()
                return {
                    'status': 'no_data_in_csv',
                    'message': f'No reports found in CSV for {report_date}'
                }
            
            # For reimport, delete existing records for this date first
            existing_count = self.db.query(SPCReport).filter(SPCReport.report_date == report_date).count()
            if existing_count > 0:
                logger.info(f"Removing {existing_count} existing records for {report_date}")
                self.db.query(SPCReport).filter(SPCReport.report_date == report_date).delete()
                self.db.commit()
            
            # Store reports with enhanced duplicate detection for reimport
            stored_counts = self._store_reports(result['reports'], report_date, is_reimport=True)
            
            # Update log
            log.success = True
            log.completed_at = datetime.utcnow()
            log.tornado_reports = stored_counts['tornado']
            log.wind_reports = stored_counts['wind'] 
            log.hail_reports = stored_counts['hail']
            log.total_reports = sum(stored_counts.values())
            
            self.db.commit()
            
            logger.info(f"Successfully reimported {log.total_reports} SPC reports for {report_date}")
            
            return {
                'status': 'success',
                'reports_ingested': log.total_reports,
                'tornado': log.tornado_reports,
                'wind': log.wind_reports,
                'hail': log.hail_reports
            }
            
        except Exception as e:
            self.db.rollback()
            logger.error(f"Error reimporting SPC reports for {report_date}: {e}")
            
            log.error_message = str(e)
            log.completed_at = datetime.utcnow()
            self.db.commit()
            
            return {
                'status': 'error',
                'error': str(e)
            }
    
    def _bulk_insert_reports(self, reports: List[Dict], report_date: date) -> Dict[str, int]:
        """
        Bulk insert reports using raw SQL with conflict resolution for reimport
        Bypasses ORM constraints to ensure successful reimport
        """
        from sqlalchemy import text
        import json
        
        tornado_count = wind_count = hail_count = stored_count = 0
        
        try:
            # Process reports in smaller batches to avoid memory issues
            batch_size = 50
            for i in range(0, len(reports), batch_size):
                batch = reports[i:i + batch_size]
                
                # Prepare bulk insert values
                values = []
                for report in batch:
                    # Generate hash with improved normalization
                    lat_str = str(report.get('latitude', '')) if report.get('latitude') is not None else ''
                    lon_str = str(report.get('longitude', '')) if report.get('longitude') is not None else ''
                    mag_str = str(report['magnitude']) if report['magnitude'] else '{}'
                    
                    hash_data = f"{report['report_date']}|{report['report_type']}|{report['time_utc']}|{report['location']}|{report['county']}|{report['state']}|{lat_str}|{lon_str}|{mag_str}"
                    clean_hash_data = hash_data.replace('\x00', '').replace('\r', '').replace('\n', ' ')
                    row_hash = hashlib.sha256(clean_hash_data.encode('utf-8')).hexdigest()
                    
                    # Convert magnitude dict to JSON string
                    magnitude_json = json.dumps(report['magnitude']) if report['magnitude'] else '{}'
                    
                    values.append({
                        'report_date': report['report_date'],
                        'report_type': report['report_type'],
                        'time_utc': report['time_utc'],
                        'location': report['location'],
                        'county': report['county'],
                        'state': report['state'],
                        'latitude': report.get('latitude'),
                        'longitude': report.get('longitude'),
                        'magnitude': magnitude_json,
                        'row_hash': row_hash,
                        'created_at': datetime.utcnow(),
                        'updated_at': datetime.utcnow()
                    })
                
                # Execute bulk insert with ON CONFLICT DO NOTHING
                if values:
                    sql = text("""
                        INSERT INTO spc_reports (
                            report_date, report_type, time_utc, location, county, state,
                            latitude, longitude, magnitude, row_hash, created_at, updated_at
                        ) VALUES (
                            :report_date, :report_type, :time_utc, :location, :county, :state,
                            :latitude, :longitude, :magnitude, :row_hash, :created_at, :updated_at
                        ) ON CONFLICT (row_hash) DO NOTHING
                    """)
                    
                    result = self.db.execute(sql, values)
                    batch_stored = result.rowcount
                    stored_count += batch_stored
                    
                    # Count by type
                    for report in batch:
                        if report['report_type'] == 'tornado':
                            tornado_count += 1
                        elif report['report_type'] == 'wind':
                            wind_count += 1
                        elif report['report_type'] == 'hail':
                            hail_count += 1
                    
                    self.db.commit()
                    logger.info(f"Batch {(i//batch_size)+1}: inserted {batch_stored}/{len(batch)} reports")
            
            logger.info(f"Bulk insert complete: {stored_count} total reports stored")
            
            return {
                'tornado': tornado_count,
                'wind': wind_count, 
                'hail': hail_count,
                'total': stored_count
            }
            
        except Exception as e:
            self.db.rollback()
            logger.error(f"Error in bulk insert: {e}")
            raise

    def get_ingestion_stats(self) -> Dict:
        """Get SPC ingestion statistics"""
        return {
            'total_reports': self.db.query(SPCReport).count(),
            'last_ingestion': self.db.query(SPCIngestionLog).order_by(SPCIngestionLog.started_at.desc()).first(),
            'successful_ingestions': self.db.query(SPCIngestionLog).filter(SPCIngestionLog.success == True).count(),
            'failed_ingestions': self.db.query(SPCIngestionLog).filter(SPCIngestionLog.success == False).count()
        }