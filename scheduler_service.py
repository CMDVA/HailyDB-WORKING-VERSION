"""
Scheduler Service for HailyDB v2.0 Core Upgrade
Provides operation logging and error recovery for autonomous ingestion
"""
import logging
from datetime import datetime
from typing import Dict, Optional
from app import db
from models import SchedulerLog

logger = logging.getLogger(__name__)

class SchedulerService:
    """
    Service to log and track all ingestion operations
    Maintains audit trail for autonomous system monitoring
    """
    
    def __init__(self, db):
        self.db = db
    
    def log_operation_start(self, operation_type: str, trigger_method: str = "manual", 
                          metadata: Optional[Dict] = None) -> SchedulerLog:
        """
        Start logging an operation
        Returns the log entry for completion tracking
        """
        try:
            log_entry = SchedulerLog(
                operation_type=operation_type,
                trigger_method=trigger_method,
                started_at=datetime.utcnow(),
                operation_metadata=metadata or {}
            )
            
            self.db.session.add(log_entry)
            self.db.session.commit()
            
            logger.info(f"Started operation {operation_type} (trigger: {trigger_method})")
            return log_entry
            
        except Exception as e:
            logger.error(f"Failed to log operation start: {e}")
            self.db.session.rollback()
            raise
    
    def log_operation_complete(self, log_entry: SchedulerLog, success: bool,
                             records_processed: int = 0, records_new: int = 0,
                             error_message: str = None, error_details: dict = None,
                             failed_alert_ids: list = None, duplicate_count: int = 0,
                             http_status_code: int = None, api_response_size: int = None,
                             processing_duration: float = None):
        """
        Complete an operation log entry with simplified approach to avoid PostgreSQL issues
        """
        try:
            # Use direct database connection to bypass SQLAlchemy entirely
            import psycopg2
            import os
            
            # Get database URL and parse it
            database_url = os.environ.get('DATABASE_URL')
            
            # Direct PostgreSQL connection
            conn = psycopg2.connect(database_url)
            cursor = conn.cursor()
            
            # Simple parameterized update
            cursor.execute("""
                UPDATE scheduler_logs 
                SET completed_at = NOW(), success = %s, records_processed = %s, records_new = %s, error_message = %s
                WHERE id = %s
            """, (success, records_processed or 0, records_new or 0, error_message, log_entry.id))
            
            conn.commit()
            cursor.close()
            conn.close()
            
            status = "SUCCESS" if success else "FAILED"
            details = f"(processed: {records_processed or 0}, new: {records_new or 0})"
            
            logger.info(f"Completed operation {log_entry.operation_type}: {status} {details}")
            
        except Exception as e:
            logger.error(f"Direct PostgreSQL completion failed: {e}")
            # Ultimate fallback: mark as completed without details
            try:
                import psycopg2
                import os
                database_url = os.environ.get('DATABASE_URL')
                conn = psycopg2.connect(database_url)
                cursor = conn.cursor()
                cursor.execute("UPDATE scheduler_logs SET completed_at = NOW(), success = %s WHERE id = %s", 
                             (success, log_entry.id))
                conn.commit()
                cursor.close()
                conn.close()
                logger.info(f"Fallback completion successful for {log_entry.operation_type}")
            except Exception as final_error:
                logger.error(f"All completion methods failed: {final_error}")
    
    def _determine_detailed_status(self, success: bool, records_processed: int, 
                                 records_new: int, error_message: str = None) -> str:
        """Determine detailed status based on operation results"""
        if not success:
            if error_message:
                if 'network' in error_message.lower() or 'timeout' in error_message.lower():
                    return 'failed_network'
                elif 'database' in error_message.lower() or 'sql' in error_message.lower():
                    return 'failed_technical'
                else:
                    return 'failed_data'
            return 'failed_unknown'
        
        # Success cases
        if records_new == 0 and records_processed > 0:
            return 'success_no_new_data'
        elif records_new > 0:
            return 'success_with_new_data'
        else:
            return 'success_empty'
    
    def get_recent_operations(self, hours: int = 24, operation_type: str = None) -> list:
        """
        Get recent operation logs for monitoring
        """
        from datetime import timedelta
        
        cutoff = datetime.utcnow() - timedelta(hours=hours)
        query = SchedulerLog.query.filter(SchedulerLog.started_at >= cutoff)
        
        if operation_type:
            query = query.filter(SchedulerLog.operation_type == operation_type)
        
        return query.order_by(SchedulerLog.started_at.desc()).all()
    
    def get_operation_stats(self) -> Dict:
        """
        Get operation statistics for health monitoring
        """
        from datetime import timedelta
        
        cutoff_24h = datetime.utcnow() - timedelta(hours=24)
        
        recent_logs = SchedulerLog.query.filter(
            SchedulerLog.started_at >= cutoff_24h
        ).all()
        
        stats = {
            'total_operations_24h': len(recent_logs),
            'successful_operations_24h': len([log for log in recent_logs if log.success]),
            'failed_operations_24h': len([log for log in recent_logs if not log.success]),
            'operations_by_type': {},
            'last_successful_operations': {}
        }
        
        # Group by operation type
        for log in recent_logs:
            op_type = log.operation_type
            if op_type not in stats['operations_by_type']:
                stats['operations_by_type'][op_type] = {'total': 0, 'successful': 0, 'failed': 0}
            
            stats['operations_by_type'][op_type]['total'] += 1
            if log.success:
                stats['operations_by_type'][op_type]['successful'] += 1
            else:
                stats['operations_by_type'][op_type]['failed'] += 1
        
        # Last successful operation for each type
        for op_type in ['nws_poll', 'spc_poll', 'spc_match', 'ai_enrich']:
            try:
                last_success = SchedulerLog.query.filter(
                    SchedulerLog.operation_type == op_type,
                    SchedulerLog.success == True
                ).order_by(SchedulerLog.completed_at.desc()).first()
                
                if last_success and last_success.completed_at:
                    stats['last_successful_operations'][op_type] = last_success.completed_at.isoformat()
            except Exception as e:
                logger.warning(f"Could not fetch last operation for {op_type}: {e}")
                continue
        
        return stats
    
    def cleanup_old_logs(self, days_to_keep: int = 30):
        """
        Clean up old operation logs to prevent database bloat
        """
        from datetime import timedelta
        
        cutoff = datetime.utcnow() - timedelta(days=days_to_keep)
        
        deleted_count = SchedulerLog.query.filter(
            SchedulerLog.started_at < cutoff
        ).delete()
        
        self.db.session.commit()
        logger.info(f"Cleaned up {deleted_count} old scheduler logs (older than {days_to_keep} days)")
        
        return deleted_count